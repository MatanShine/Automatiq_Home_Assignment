# CyFortis Training Assistant

AI assistant that answers **cybersecurity training** questions for both **employees** and the **CISO** using natural language.

The agent can:

- Tell an employee whether theyâ€™ve finished the training and which videos are missing.
- Let the CISO inspect a single employeeâ€™s status and training summary.
- Let the CISO query all employees by status and see overall statistics (min/max/avg time, fastest, slowest).
- Enforce authentication (ID + name) before any training data is exposed.
- Stay strictly on-topic (security training only) and operate in a **read-only** way.

---

## ğŸš€ Quickstart

### 1. Prerequisites

- Docker & Docker Compose
- OpenAI API key

### 2. Environment file

In the project root:

```bash
cp .env.example .env
```
Then edit .env with:
```bash
OPENAI_API_KEY=your_token_here
```
The backend reads these variables to configure the LLM client.


### 3. Run the full stack
From the project root:
```bash
docker-compose up --build
```
This starts:
- Backend: FastAPI app (Python) on http://localhost:8000
- Frontend: React/Vite app (TypeScript) on http://localhost:3000

Both services use bind mounts and hot reload for a smooth dev experience.

### 4. Where to Access
- Frontend (chat UI)
http://localhost:3000
Vite dev server with hot module replacement, reading and displaying messages from the backend.
- Backend (API) http://localhost:8000 FastAPI with:
    - POST /chat â€“ single endpoint for all chat interactions.
    - Optional FastAPI docs: http://localhost:8000/docs


### 5. Repo structure
```bash
.
â”œâ”€ backend/
â”‚  â”œâ”€ app/
â”‚  â”‚  â”œâ”€ main.py                  # FastAPI app & router registration
â”‚  â”‚  â”œâ”€ api/
â”‚  â”‚  â”‚  â””â”€ endpoints.py          # /chat endpoint
â”‚  â”‚  â”œâ”€ db/
â”‚  â”‚  â”‚  â””â”€ queries.py            # SQLite data access helpers
â”‚  â”‚  â”œâ”€ services/
â”‚  â”‚  â”‚  â”œâ”€ llm_client_setup.py   # LLM client wiring
â”‚  â”‚  â”‚  â”œâ”€ llm_config.py         # model name, system prompts, constants
â”‚  â”‚  â”‚  â”œâ”€ tools.py              # tool schemas/metadata
â”‚  â”‚  â”‚  â””â”€ llm_tool_handlers.py  # Python implementations of tools
â”‚  â”‚  â””â”€ models/                  # Pydantic request/response models
â”‚  â””â”€ data/
â”‚     â””â”€ employees.db             # SQLite DB with employees & training data
â”‚
â”œâ”€ frontend/
â”‚  â”œâ”€ src/
â”‚  â”‚  â”œâ”€ App.tsx                  # App root and routing
â”‚  â”‚  â”œâ”€ components/
â”‚  â”‚  â”‚  â”œâ”€ Chat.tsx              # Chat UI
â”‚  â”‚  â”‚  â””â”€ MessageBubble.tsx     # Chat message rendering
â”‚  â”‚  â”œâ”€ hooks/
â”‚  â”‚  â”‚  â””â”€ useChatHistory.ts     # Session & history management
â”‚  â”‚  â”œâ”€ context/
â”‚  â”‚  â”‚  â””â”€ ThemeContext.tsx      # Light/dark theme
â”‚  â”‚  â””â”€ api/
â”‚  â”‚     â””â”€ client.ts             # Typed client for /chat
â”‚  â””â”€ vite.config.ts
â”‚
â”œâ”€ assignment/                    # Original home-assignment materials
â”œâ”€ docker-compose.yml
â”œâ”€ .env.example
â””â”€ README.md
```
### 6. High-Level Architecture

Single /chat endpoint

All conversations (employee or CISO) go through one HTTP endpoint:
1. The frontend sends:messages (history)
	-	Optional employee_id and employee_name
2.	The backend:
	- Validates authentication (ID + name)
	- Calls the LLM with system and tool definitions
	- Lets the LLM pick tools (e.g., â€œget_employee_statusâ€, â€œlist_employees_by_statusâ€)
	- Executes the corresponding handler (read-only DB queries)
	- Returns a ChatResponse back to the frontend

This design keeps:
	- Auth checks in one place.
	- Tool chaining and history formatting centralized.
	- The frontend completely unaware of database internals.
### 7. DataBase
- All DB access is done via simple helper functions in backend/app/db/queries.py using Pythonâ€™s sqlite3.
- There is deliberately no ORM to keep the assignment **focused and transparent**.

### 8. Authentication flow:

The assignment requires ID + employee name before any meaningful answer. The flow is:
1.	Unauthenticated user:
	- The LLM is instructed to ask for name and ID first.
	- Until both are provided and verified, only generic prompts are allowed (â€œPlease provide your name and IDâ€).
2.	Verification:
	- Backend checks (employee_id, employee_name) via employee_exists_in_database in db/queries.py.
	- If the record doesnâ€™t exist, the LLM is instructed to respond as â€œunknown userâ€.
3.  Session handling:
	-	The frontend stores employee_id and employee_name in useChatHistory per session.
	-	Each /chat request includes these values so stateless backend instances can authenticate every call.
4.	Role detection (Employee vs CISO):
	-	The DB also tracks whether a user is a CISO (e.g., an is_ciso flag).
	-	After authentication, the backend computes is_ciso and includes it in the context passed to the LLM.
	-	Tool availability then depends on this flag (see next section).

### 9. Authorization rules:
-   Unauthorized:
    -   Can only query their name and id to "login".
-	Employees:
    -	Can only query their own training status and videos.
    -	No access to organization-wide stats or other employees.
-	CISO:
	â€¢	Can query any employee by name and ID.
	â€¢	Can list all employees by status.
	â€¢	Can request global statistics.

All of this is enforced in the tool handlers, not in the frontend, to prevent bypassing via custom clients.

### 10. LLM & Tooling Design

The backend uses an OpenAI-compatible LLM (default: gpt-4o-mini **for maximum speed and minimum costs**) configured in:
-	llm_config.py â€“ model name, system prompts, and instruction strings.
-	llm_client_setup.py â€“ instantiated LLM client using env variables.

**Tools**

Tools are described in services/agent_tools/tools.py and implemented in llm_tool_handlers.py.

Each tool:
1.	Has a schema (name, description, parameters).
2.	Is wired to a handler that:
	-	Validates inputs (e.g., employee must exist).
	-	Reads from SQLite via db/queries.py.
	-	Returns a structured result (Python dict) for the LLM to verbalize.

This separation lets us:
-	Change prompts or models without touching business logic.
-	Add new capabilities by just:
    -	Declaring the tool schema.
    -	Implementing the handler.
	-	Registering it in TOOL_HANDLERS.

### 11. Scaling & Performance
- 	Stateless backend:
	-	/chat requests include all necessary context (history + auth).
	-	Any FastAPI instance can handle any request; no sticky sessions.
	-	Easily run multiple replicas behind a load balancer in production.
-	Database considerations:
	-	SQLite is sufficient for this assignment and easy to package in Docker.
	-	For higher scale, the same query abstraction could be backed by Postgres/MySQL.
-   Caching:
	-	we already cache expensive CISO statistics queries and llm responses to reduce time and costs.
-	Optimizations (future):
	-	Stream partial LLM responses to the frontend for better UX.

### 12. Known Limitations & Future Work
-	No persistent sessions across browser tabs or devices (history is in-memory on the frontend).
-	No admin UI for browsing training stats â€“ CISO interacts only via chat.
-	No automated tests checked in yet; strategy exists but needs implementation.
-	Error handling is basic and could be extended (e.g., nicer error boundaries in the UI).


### 13. Summary

This project implements the CyFortis AI assistant as specified:
-	Employee & CISO natural language interface over training data.
-	Authentication enforced via name + ID.
-	Clear separation of concerns: React UI, FastAPI backend, LLM tools, SQLite database.
-	Read-only, guarded LLM usage focused strictly on the cybersecurity training task.
-	A single /chat endpoint that centralizes conversation logic, tools, and auth.

The design emphasizes safety, extensibility, and developer ergonomics, while staying faithful to the assignment constraints.
